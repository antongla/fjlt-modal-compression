{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Test the distance preservation properties of FJLT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from random import sample\n",
    "import sys\n",
    "sys.path.append('../../../utils')\n",
    "from TurboFJLT import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path=\"../../../data/fine_airfoil_cascade.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FJLT:\n",
    "    def __init__(self, vec_dim, snapshots_dim, max_distortion):\n",
    "        self.vec_dim = vec_dim\n",
    "        self.snapshots_dim = snapshots_dim\n",
    "        self.max_distortion = max_distortion\n",
    "        self.embedding_dim = 8*int(np.log(self.snapshots_dim)/self.max_distortion/self.max_distortion)\n",
    "        self.sparsity = np.log(self.snapshots_dim)*np.log(self.snapshots_dim)/self.vec_dim\n",
    "        self.__gen_matrices()\n",
    "\n",
    "    def __str__(self):\n",
    "        return (\n",
    "            \"State vector dimensions:        {vec_dim}\\n\"\n",
    "            \"Number of embedded snapshots:   {snapshots_dim}\\n\"\n",
    "            \"Maximum distortion:             {max_distortion}\\n\"\n",
    "            \"Embedding dimension:            {embedding_dim}\\n\"\n",
    "            \"Sparsity:                       {sparsity:.5e}\\n\"\n",
    "            \"Generating matrices took:       {perf_counter_matrices:.5f} seconds\"\n",
    "        ).format(**self.__dict__)\n",
    "\n",
    "    def __gen_matrices(self):\n",
    "        t0    = time.time()\n",
    "        self.P,self.s,self.D = fjlt_Matrices(self.vec_dim,\n",
    "                                             self.snapshots_dim,\n",
    "                                             self.embedding_dim,\n",
    "                                             self.sparsity)\n",
    "        t1    = time.time()\n",
    "        self.perf_counter_matrices = t1-t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class to read in data for turbomachinery problem\n",
    "class TurboHDF5Reader:\n",
    "    def __init__(self, file):\n",
    "        self.file = file\n",
    "        self.__load_parameters()\n",
    "        return None\n",
    "\n",
    "    def __str__(self):\n",
    "        return (\"Extracting cascade data from: {file}\\n\"\n",
    "                \"TEMPORAL\\n\"\n",
    "                \"Number of snapshots:          {num_snapshots}\\n\"\n",
    "                \"Timestep:                     {timestep:.5e}\\n\"\n",
    "                \"SPATIAL\\n\"\n",
    "                \"Number of passages:           {num_passages}\\n\"\n",
    "                \"Number of regions:            {num_regions}\\n\"\n",
    "                \"Region shapes:                {regions_shape}\\n\"\n",
    "                \"State vector dimension:       {state_dim}\"\n",
    "               ).format(**self.__dict__)\n",
    "\n",
    "    def __load_parameters(self):\n",
    "        # The number of degrees of freedom is 4 in this case\n",
    "        dofs = 4\n",
    "        with h5.File(self.file, 'r') as f:\n",
    "            self.keys = list(f.keys())\n",
    "\n",
    "            # Temporal parameters\n",
    "            self.num_snapshots = len(self.keys)-2\n",
    "            self.timestep = f[self.keys[1]].attrs['t'][0]-f[self.keys[0]].attrs['t'][0]\n",
    "\n",
    "            # Spatial parameters\n",
    "            self.num_regions = len(f[\"/{}/field\".format(self.keys[0])])\n",
    "            self.num_passages = self.num_regions-2\n",
    "            self.regions_shape = f[\"/{}/field\".format(self.keys[0])].attrs[\"param\"]\n",
    "            self.state_dim = ((\n",
    "                self.regions_shape[0] +\n",
    "                self.regions_shape[2])*(self.regions_shape[3]-1)*self.num_passages +\n",
    "                self.regions_shape[1]*self.regions_shape[3]\n",
    "                )*self.num_passages*dofs\n",
    "\n",
    "        return None\n",
    "\n",
    "    def __load_snapshot(self, h5_file, snapshot_id):\n",
    "        qs = []\n",
    "        for region in range(self.num_regions):\n",
    "            qs.append(h5_file[\"/{}/field/{}\".format(self.keys[snapshot_id], region)][()].flatten())\n",
    "        Q = np.hstack(qs)\n",
    "        return Q\n",
    "\n",
    "    def __load_snapshot_chunk(self, snap_chunk_list):\n",
    "        with h5.File(self.file, 'r') as f:\n",
    "            snapshots = [self.__load_snapshot(f, snap_ind) for snap_ind in snap_chunk_list]\n",
    "        return snapshots\n",
    "\n",
    "    def __setup_chunking(self, snapshot_list, chunk_dim):\n",
    "        num_full_chunks = len(snapshot_list)//chunk_dim\n",
    "        self.snapshot_chunks_inds = [snapshot_list[i*chunk_dim:(i+1)*chunk_dim] for i in range(num_full_chunks)]\n",
    "        if len(snapshot_list)%chunk_dim != 0:\n",
    "            self.snapshot_chunks_inds.append(snapshot_list[num_full_chunks*chunk_dim:])\n",
    "        # Extra params for data\n",
    "        self.chunk_dim = chunk_dim\n",
    "        self.num_chunks = len(self.snapshot_chunks_inds)\n",
    "        return None\n",
    "\n",
    "    def reset_chunked_loading(self, snapshot_list, chunks_dim):\n",
    "        self.q_chunk = None\n",
    "        assert np.all(np.array(snapshot_list)>=0) and np.all(np.array(snapshot_list)<self.num_snapshots), \"Index out of range in snapshot list\"\n",
    "        chunks_dim = len(snapshot_list) if chunks_dim > len(snapshot_list) else chunks_dim\n",
    "        self.__setup_chunking(snapshot_list, chunks_dim)\n",
    "        self.__current_index = -1\n",
    "        return None\n",
    "\n",
    "    def load_next(self):\n",
    "        self.__current_index += 1\n",
    "\n",
    "        in_chunk_index = self.__current_index%self.chunk_dim\n",
    "        chunk_index = self.__current_index//self.chunk_dim\n",
    "\n",
    "        # Load only when we move to a new chunk\n",
    "        if in_chunk_index == 0:\n",
    "            self.q_chunk = self.__load_snapshot_chunk(self.snapshot_chunks_inds[chunk_index])\n",
    "        return self.q_chunk[in_chunk_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = TurboHDF5Reader(data_path)\n",
    "print(reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Calculate the FJLT for pair of vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fjlt_distortion_statistics(num_snapshot_linkage, num_samples, memory_chunk_size=50):\n",
    "    fjlt = FJLT(reader.state_dim, num_snapshot_linkage, 0.01)\n",
    "    print(fjlt)\n",
    "\n",
    "    # Generate non-repeating pairs of indices\n",
    "    random_pairs = num_samples\n",
    "    chunks_dim = memory_chunk_size\n",
    "\n",
    "    tuple_set = set()\n",
    "    for _ in range(random_pairs):\n",
    "        i = np.random.randint(0,reader.num_snapshots-1)\n",
    "        j = np.random.randint(i+1,reader.num_snapshots)\n",
    "        tup = (i, j)\n",
    "        tuple_set.add(tup)\n",
    "\n",
    "    snapshot_list = []\n",
    "    for tup in tuple_set:\n",
    "        snapshot_list.append(tup[0])\n",
    "        snapshot_list.append(tup[1])\n",
    "\n",
    "    reader.reset_chunked_loading(snapshot_list, chunks_dim)\n",
    "\n",
    "    # Compute pairwise distortion\n",
    "    q_pair = [None, None]\n",
    "    b_pair = [None, None]\n",
    "    distortion = []\n",
    "    dist_xy = []\n",
    "    smallest_vec_len = []\n",
    "    for i, _ in enumerate(tqdm(snapshot_list)):\n",
    "        q_pair[i%2] = reader.load_next()\n",
    "        b_pair[i%2] = applyFJLT(q_pair[i%2],fjlt.P,fjlt.s,fjlt.D)\n",
    "        if i%2==1:\n",
    "            xy  = np.linalg.norm(q_pair[0]-q_pair[1])\n",
    "            XY  = np.linalg.norm(b_pair[0]-b_pair[1])\n",
    "            dist_xy.append(xy)\n",
    "            distortion.append(abs(xy-XY)/xy*100)\n",
    "            vec_0_len = np.linalg.norm(q_pair[0])\n",
    "            vec_1_len = np.linalg.norm(q_pair[1])\n",
    "            smallest_vec_len.append(min([vec_0_len, vec_1_len]))\n",
    "\n",
    "    # Free a few 100 MBs of memory\n",
    "    reader.reset_chunked_loading(snapshot_list, chunks_dim)\n",
    "\n",
    "    return distortion, dist_xy, smallest_vec_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_linking_snapshots = [2, 4, 6]\n",
    "for n_sp in num_linking_snapshots:\n",
    "    distortion, dist_xy, smallest_vec_len = fjlt_distortion_statistics(n_sp, num_samples=60000)\n",
    "    with h5.File(\"../data/distortion_metrics_{}_linking_snapshots.h5\".format(n_sp), 'w') as f:\n",
    "        f.create_dataset(\"/distortion\", data=distortion)\n",
    "        f.create_dataset(\"/dist_xy\", data=dist_xy)\n",
    "        f.create_dataset(\"/vec_len\", data=smallest_vec_len)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
